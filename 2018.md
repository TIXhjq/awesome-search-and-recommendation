## 2018


### ACL'18

1. Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe Zhang, Dinghan Shen, Xinyuan Zhang, Ricardo Henao, Lawrence Carin. [Joint Embedding of Words and Labels for Text Classification](http://people.ee.duke.edu/~lcarin/acl2018_Guoyin.pdf), *ACL*, 2018.

   > 文章采用matching的方式来做text classifiation，主要创新点在于将label也embedding成vector向量。对word embedding的sequence与label vector进行attention计算，但考虑到句子的spatial信息(phrases)，模型没有直接将attention后的向量相加，而是以sliding window的方法对句子进行划分窗口(长度为2r)，以类似CNN的方法（单个filter）计算一个convolution vector，max pooling后得到窗口下的score值，所有窗口下的结果经过concat之后过softmax进行分类。有一个trick是label vectors作为目标向量应该有明确的分类效果，这里计算分类中常用的cross-entropy loss作为正则项。总的来看，模型是一个matching + classification的结合。

### ICLR'18

1. Kevin Clark, Minh-Thang Luong, Christopher D. Manning, Quoc V. Le. [Semi-Supervised Sequence Modeling with Cross-View Training](https://arxiv.org/pdf/1809.08370), *ICLR*, 2018.

   > 当前NLP领域的半监督学习都基于pretraining, 比如word2vec训练词向量，ELMo/BERT训练sentence encoder。但缺点就是pretraining没有用到label信息。本文采用self-traning的思路，即利用supervised model预测unlabeled data，再利用这些data训练model。为了增强鲁棒性，提出了cross-view training的方法，就是在训练原有supervised model的同时用unlabeled data训练一些auxiliary modules，有监督部分和无监督部分交叉进行。这些module利用restricted view of input（部分特征）来逼近supervised model预测出的label分布，这样能提高共享representation layer的质量，进而优化原有model。


